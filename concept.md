# Предобработка данных NLP

## Токенизация
 Разбиение текста на токены (слова, предложения, символы)
 Библиотеки: NLTK, spaCy, Tokenizers

## Удаление стоп-слов
 Удаление часто встречающихся слов, которые не несут смысловой нагрузки
 Примеры: "и", "в", "на"
 Библиотеки: NLTK, spaCy

## Приведение к нижнему регистру
 Преобразование всех символов в тексте к нижнему регистру
 Помогает избавиться от различий в написании слов с заглавных и строчных букв
 Метод .lower() в Python

## Лемматизация и стемминг
 Преобразование слов к их основным формам
 Лемматизация - поиск леммы слова в словаре
 Стемминг - обрезание слова до основы
 Библиотеки: NLTK, spaCy

# Библиотеки векторизации текста

## CountVectorizer (Bag of Words)
 Подсчет количества встречающихся слов в тексте
 Создание разреженной матрицы
 Преобразование текста в числовой формат
 Scikit-learn

## TfidfVectorizer
 Term Frequency-Inverse Document Frequency
 Оценивает важность слова в контексте текста и корпуса
 Уменьшает вес часто встречающихся слов
 Scikit-learn

## Word2Vec
 Преобразует слова в векторное пространство
 Сохраняет семантические отношения между словами
 Mikolov et al. (2013)

## GloVe
 Global Vectors for Word Representation
 Создает векторное представление слов на основе матрицы совстречаемости
 Stanford NLP Group

# Основные методы работы с векторизаторами текста
1. Создание экземпляра векторизатора: 
Python

tfidf_vectorizer = TfidfVectorizer()

2. Обучение векторизатора на корпусе текста:
Python

tfidf_vectorizer.fit(corpus)

3. Преобразование текста в числовую матрицу:
Python

X = tfidf_vectorizer.transform(corpus)

4. Получение признаков и их значений:
Python

feature_names = tfidf_vectorizer.get_feature_names()
print(feature_names)
print(X.toarray())

5. Использование векторизатора для векторизации новых текстов: 
Python

new_text = ["New text to vectorize"]
new_text_vectorized = tfidf_vectorizer.transform(new_text)

Это основные шаги по предобработке текста и использованию библиотек векторизации для задач анализа текста в NLP.
